{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import euclidean_distances\n",
    "from tensorflow.keras.applications import ResNet50\n",
    "from tensorflow.keras.applications.resnet50 import preprocess_input\n",
    "from tensorflow.keras.preprocessing.image import load_img, img_to_array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1st Task"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Rank Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 13 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x000001F1DA1D89A0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 99ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step\n",
      "[0.0, 0.0, 0.0]\n",
      "[0.0, 0.0, 0.0]\n",
      "[0.0, 0.0, 0.0]\n"
     ]
    }
   ],
   "source": [
    "# Directory containing the images\n",
    "image_folder = \"animals\"\n",
    "\n",
    "# Feature extraction model\n",
    "model = ResNet50(weights=\"imagenet\", include_top=False, pooling=\"avg\")\n",
    "\n",
    "def feature_extraction(image_path):\n",
    "    \"\"\"\n",
    "    Extract feature vector from an image.\n",
    "    :param image_path: Path to the image\n",
    "    :return: Feature vector\n",
    "    \"\"\"\n",
    "    image = load_img(image_path, target_size=(224, 224))  # Resize image to match model input size\n",
    "    image_array = img_to_array(image)  # Convert to array\n",
    "    image_array = np.expand_dims(image_array, axis=0)  # Add batch dimension\n",
    "    image_array = preprocess_input(image_array)  # Preprocess for ResNet50\n",
    "    feature_vector = model.predict(image_array)[0]  # Extract features\n",
    "    return feature_vector\n",
    "\n",
    "def compute_distance(feature_vector_1, feature_vector_2):\n",
    "    \"\"\"\n",
    "    Compute the Euclidean distance between two feature vectors.\n",
    "    :param feature_vector_1: First feature vector\n",
    "    :param feature_vector_2: Second feature vector\n",
    "    :return: Euclidean distance\n",
    "    \"\"\"\n",
    "    return euclidean_distances([feature_vector_1], [feature_vector_2])[0][0]\n",
    "\n",
    "def compute_similarity(feature_vector_1, feature_vector_2):\n",
    "    \"\"\"\n",
    "    Compute similarity based on the Euclidean distance.\n",
    "    :param feature_vector_1: First feature vector\n",
    "    :param feature_vector_2: Second feature vector\n",
    "    :return: Similarity score (higher means more similar)\n",
    "    \"\"\"\n",
    "    distance = compute_distance(feature_vector_1, feature_vector_2)\n",
    "    return 1 / (1 + distance)  # Invert distance for similarity score\n",
    "\n",
    "# Extract features for all images in the folder\n",
    "image_paths = [os.path.join(image_folder, img) for img in os.listdir(image_folder) if img.endswith('.jpg')]\n",
    "features = {}\n",
    "\n",
    "for image_path in image_paths:\n",
    "    features[image_path] = feature_extraction(image_path)\n",
    "    \n",
    "# Compute pairwise similarity\n",
    "similarity_matrix = {}\n",
    "\n",
    "#for (img1, img2), similarity in similarity_matrix.items():\n",
    "#    print(f\"Similarity between {img1} and {img2}: {similarity:.4f}\")\n",
    "\n",
    "# Extract image paths from the dictionary keys\n",
    "image_paths = list(features.keys())\n",
    "\n",
    "# Initialize a 2D list (numpy array) for the similarity matrix\n",
    "num_images = len(image_paths)\n",
    "similarity_2d_list = np.zeros((num_images, num_images))\n",
    "\n",
    "# Fill the 2D list with similarity scores\n",
    "for i, path1 in enumerate(image_paths):\n",
    "    for j, path2 in enumerate(image_paths):\n",
    "        similarity = similarity_matrix.get((path1, path2), 0)  # Default to 0 if no similarity found\n",
    "        similarity_2d_list[i][j] = similarity\n",
    "\n",
    "# Optionally, convert the numpy array to a regular list (if needed)\n",
    "similarity_2d_list = similarity_2d_list.tolist()\n",
    "\n",
    "# Print the 2D list (optional)\n",
    "for row in similarity_2d_list:\n",
    "    print(row)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "tuple index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[15], line 48\u001b[0m\n\u001b[0;32m     45\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m normalized_matrix\n\u001b[0;32m     47\u001b[0m L \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2\u001b[39m  \u001b[38;5;66;03m# Top 2 neighbors\u001b[39;00m\n\u001b[1;32m---> 48\u001b[0m normalized_matrix \u001b[38;5;241m=\u001b[39m \u001b[43mrank_normalization\u001b[49m\u001b[43m(\u001b[49m\u001b[43msimilarity_matrix\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mL\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     49\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNormalized Similarity Matrix:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     50\u001b[0m \u001b[38;5;28mprint\u001b[39m(normalized_matrix)\n",
      "Cell \u001b[1;32mIn[15], line 18\u001b[0m, in \u001b[0;36mrank_normalization\u001b[1;34m(similarity_matrix, L)\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;66;03m# Initialize the normalized similarity matrix with the same shape\u001b[39;00m\n\u001b[0;32m     16\u001b[0m normalized_matrix \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mzeros_like(similarity_matrix)\n\u001b[1;32m---> 18\u001b[0m num_objects \u001b[38;5;241m=\u001b[39m \u001b[43msimilarity_matrix\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;66;03m# Iterate through each pair (i, j) in the matrix\u001b[39;00m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_objects):\n",
      "\u001b[1;31mIndexError\u001b[0m: tuple index out of range"
     ]
    }
   ],
   "source": [
    "def rank_normalization(similarity_matrix, L):\n",
    "    \"\"\"\n",
    "    Perform reciprocal rank normalization on the given similarity matrix.\n",
    "\n",
    "    Args:\n",
    "    - similarity_matrix (list or np.array): A square matrix where each element A[i][j] represents the similarity between objects i and j.\n",
    "    - L (int): The top-L positions to consider in the rank normalization.\n",
    "\n",
    "    Returns:\n",
    "    - normalized_matrix (np.array): The updated similarity matrix after rank normalization.\n",
    "    \"\"\"\n",
    "    # Convert the similarity matrix into a numpy array for easier manipulation\n",
    "    similarity_matrix = np.array(similarity_matrix)\n",
    "    \n",
    "    # Initialize the normalized similarity matrix with the same shape\n",
    "    normalized_matrix = np.zeros_like(similarity_matrix)\n",
    "    \n",
    "    num_objects = similarity_matrix.shape[0]\n",
    "    \n",
    "    # Iterate through each pair (i, j) in the matrix\n",
    "    for i in range(num_objects):\n",
    "        for j in range(i + 1, num_objects):  # Avoid redundant calculations\n",
    "            # Rank position of j in the list of neighbors of i\n",
    "            rank_ij = np.argsort(similarity_matrix[i, :])[::-1].tolist().index(j)\n",
    "            \n",
    "            # Rank position of i in the list of neighbors of j\n",
    "            rank_ji = np.argsort(similarity_matrix[j, :])[::-1].tolist().index(i)\n",
    "            \n",
    "            # Apply the rank normalization formula\n",
    "            normalized_value = 2 * L - (rank_ij + rank_ji)\n",
    "            \n",
    "            # Set the normalized similarity values\n",
    "            normalized_matrix[i, j] = normalized_value\n",
    "            normalized_matrix[j, i] = normalized_value  # Symmetric assignment\n",
    "    \n",
    "    # Update the top-L positions using a stable sort (sorted in descending order)\n",
    "    for i in range(num_objects):\n",
    "        # Get indices of the top-L neighbors (after rank normalization)\n",
    "        top_L_indices = np.argsort(normalized_matrix[i, :])[::-1][:L]\n",
    "        \n",
    "        # Ensure the stability of sorting by updating only the top-L similarities\n",
    "        normalized_matrix[i, :] = 0  # Zero out all but top-L neighbors\n",
    "        normalized_matrix[i, top_L_indices] = similarity_matrix[i, top_L_indices]\n",
    "    \n",
    "    return normalized_matrix\n",
    "\n",
    "L = 2  # Top 2 neighbors\n",
    "normalized_matrix = rank_normalization(similarity_matrix, L)\n",
    "print(\"Normalized Similarity Matrix:\")\n",
    "print(normalized_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a pre-trained SqueezeNet model\n",
    "squeezenet = models.squeezenet1_1(pretrained=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1, 2, 3], [4, 5, 6], [7, 8, 9]]\n"
     ]
    }
   ],
   "source": [
    "# Original 1D list with 9 items\n",
    "original_list = [1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
    "\n",
    "# Convert to 2D list with 3 items per row\n",
    "two_d_list = [original_list[i:i+3] for i in range(0, len(original_list), 3)]\n",
    "\n",
    "print(two_d_list)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
